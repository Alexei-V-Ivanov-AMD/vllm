#!/usr/bin/env python3

#
#   This is a quick hack that produces PPL measurement by 
#   iteratively dumping the logprob vector for the single next symbol 
#   that is to be generated over the preloaded context.
#   It is actually an *inefficient* procedure because for the
#   N-token string it takes N*(preload + generation) time instead of
#   preload + N*generation
#

#   The script assumes existence of the 
#       data, generated by 'git lfs clone https://huggingface.co/datasets/wikitext'
#       vllm package installed and available
#       logprob vector is obtained with natural (base e) logarithm (a fact for vllm 
#       inference engine as of 2024-03-01)
#      

import numpy as np
from transformers import LlamaForCausalLM, LlamaTokenizer
import pandas as pd
import torch
import sys
import datetime
import json
import argparse
from vllm import LLM, SamplingParams
import math
import operator

#import pdb

#MODEL = "/data/models/llama-2-7b-chat-hf"
#MODEL = "/data/models/llama-2-13b-chat-hf"
#MODEL = "/data/models/llama-2-70b-chat-hf"

#DATA = './wikitext/wikitext-2-v1/test-00000-of-00001.parquet'

#CONTEXT_SIZE = 1024 #2048 # 4096 is impossible with vLLM (2048 is the max)

#BS = 100 # Batch Size

#TP = 1

def get_wikitext2(tokenizer):
    test=pd.read_parquet(args.data)
    test_text="\n\n".join(test['text'])
    test_enc = tokenizer(test_text)

    return test_enc, test_text
def vllm_init(args):
    llm = LLM(
        model=args.model,
        tokenizer=None,
        tensor_parallel_size=args.tensor_parallel_size,
        trust_remote_code=args.trust_remote_code,
        dtype=args.dtype,
        kv_cache_dtype=args.kv_cache_dtype,
        kv_cache_scales_path=args.kv_cache_scales_path if args.kv_cache_scales_path!='' else None,
        max_context_len_to_capture=args.context_size,
    )

    sampling_params = SamplingParams(
        n=1, #args.n,
        temperature=0.0, #if args.use_beam_search else args.temperature,
        top_p=1,
        use_beam_search=False,
        ignore_eos=True,
        max_tokens=1,
        logprobs=32000,
        presence_penalty=0.0,
        #repetition_penalty=0.000001,
        #length_penalty=0.0,

    )

    return llm, sampling_params

def vllm_predict(CONT, llm, sampl_par):
    #pdb.set_trace()
    result=llm.generate( prompt_token_ids=CONT,sampling_params=sampl_par)
    #pdb.set_trace()
    return result

def main(args: argparse.Namespace):

    print (f"### Initialising @ {datetime.datetime.now()}")
    my_ppl1=0.0
    my_ppl2=0.0

    my_tokenizer = LlamaTokenizer.from_pretrained(args.model)
    print("Loaded the tokenizer.")

    print("*** Initializing the engine.")
    my_llm, my_sampl_par = vllm_init(args)
    print(my_sampl_par)
    print("*** Initialized the engine.")

    my_test_enc, my_test_text = get_wikitext2(my_tokenizer)
    print("Loaded the test data.") 

    print(f"There are {len(my_test_text)} characters and {len(my_test_enc['input_ids'])} tokens in the test.")
    #There are 1260798 characters and 343667 tokens in the wikitext-2-v1 test set
    # (./wikitext/wikitext-2-v1/test-00000-of-00001.parquet )
    #There are 1125112 characters and 302005 tokens in the wikitext-2-v1 validation set 
    # (./wikitext/wikitext-2-v1/validation-00000-of-00001.parquet )
    # Validation and test sets of wikitext-2-v1 and wikitext-103-v1 are similar/identical ?

    my_n_samples = (len(my_test_enc['input_ids'])-args.context_size-1)//args.batch_size        # the last context lacks the next symbol ID

    print (f"### Starting generation @ {datetime.datetime.now()}")
    for c in range(my_n_samples):
        #pdb.set_trace()
        CONTEXT = []
        for r in range(args.batch_size):
            CONTEXT.append(my_test_enc['input_ids'][c*args.batch_size+r:c*args.batch_size+r+args.context_size])

        LOGITS = vllm_predict(CONTEXT, my_llm, my_sampl_par)

       
        my_ppl2=my_ppl2*c/(c+1)
        for r in range(args.batch_size):
            #pdb.set_trace()
            #potsum = 0
            #for logp in range(32000): # LOGITS[r].outputs[0].logprobs[0]:
            #    potsum += math.exp(LOGITS[r].outputs[0].logprobs[0][logp])                                        
            #print(f"@@ r={r} potsum={potsum}")

            my_ppl1 -= LOGITS[r].outputs[0].logprobs[0][my_test_enc['input_ids'][c*args.batch_size+r+args.context_size]]
            my_ppl2 -= LOGITS[r].outputs[0].logprobs[0][my_test_enc['input_ids'][c*args.batch_size+r+args.context_size]]/((c+1)*args.batch_size)
        print(f"Intermediate Perplexity estimates:\n\tPPLint1={my_ppl1/((c+1)*args.batch_size)}\n\tPPLint2={my_ppl2}")

    my_ppl1/=(my_n_samples*args.batch_size)

    print (f"### Done @ {datetime.datetime.now()}")

    print(f"Perplexity estimates:\n\tPPLmethod1={my_ppl1}\n\tPPLmethod2={my_ppl2}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
    description='Benchmark the latency of processing a single batch of '
        'requests till completion.')
    parser.add_argument('--model', type=str, default='facebook/opt-125m')
    parser.add_argument('--data', type=str, default='./wikitext/wikitext-2-v1/test-00000-of-00001.parquet')
    parser.add_argument('--context-size', type=int, default=4096)
    parser.add_argument('--kv-cache-scales-path', type=str, default='')
    parser.add_argument('--num-samples-per-task', type=int, default=1)
    parser.add_argument('--temperature', type=float, default=1.0)
    parser.add_argument('--experiment-prefix',type=str, default='solution_samples')
    parser.add_argument('--tokenizer', type=str, default=None)
    parser.add_argument('--quantization',
                        '-q',
                        choices=['awq', 'gptq', 'squeezellm', None],
                        default=None)
    parser.add_argument('--tensor-parallel-size', '-tp', type=int, default=1)
    parser.add_argument('--input-len', type=int, default=32)
    parser.add_argument('--output-len', type=int, default=128)
    parser.add_argument('--batch-size', type=int, default=8)
    parser.add_argument('--n',
                        type=int,
                        default=1,
                        help='Number of generated sequences per prompt.')
    parser.add_argument('--use-beam-search', action='store_true')
    parser.add_argument('--num-iters',
                        type=int,
                        default=3,
                        help='Number of iterations to run.')
    parser.add_argument('--trust-remote-code',
                        action='store_true',
                        help='trust remote code from huggingface')
    parser.add_argument(
        '--dtype',
        type=str,
        default='auto',
        choices=['auto', 'half', 'float16', 'bfloat16', 'float', 'float32'],
        help='data type for model weights and activations. '
        'The "auto" option will use FP16 precision '
        'for FP32 and FP16 models, and BF16 precision '
        'for BF16 models.')
    parser.add_argument('--enforce-eager',
                        action='store_true',
                        help='enforce eager mode and disable CUDA graph')
    parser.add_argument(
        "--kv-cache-dtype",
        type=str,
        choices=['auto', 'fp8_e5m2','fp8'],
        default='auto',
        help=
        'Data type for kv cache storage. If "auto", will use model data type.')
    parser.add_argument(
        '--profile',
        action='store_true',
        help='profile the generation process of a single batch')
    parser.add_argument(
        '--profile-result-dir',
        type=str,
        default=None,
        help=('path to save the pytorch profiler output. Can be visualized '
              'with ui.perfetto.dev or Tensorboard.'))
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        choices=["cuda"],
        help='device type for vLLM execution, supporting CUDA only currently.')
    args = parser.parse_args()

    main(args)

#   Example invocations:
#   ./measure_ppl_slow.py --model=/data/models/llama-2-13b-chat-hf --data=./wikitext/wikitext-2-v1/test-00000-of-00001.parquet --context-size=2048 --batch-size=1 -tp=1 --kv-cache-dtype="fp8" &> ./llama-2-13b-chat-hf_wikitext-2-v1-test_bs1xit341618xcont2048_FP8.log
#   ./measure_ppl_slow.py --model=/data/models/llama-2-7b-chat-hf --data=./wikitext/wikitext-2-v1/test-00000-of-00001.parquet --context-size=2048 --batch-size=1 -tp=1 --kv-cache-dtype="fp8" &> ./llama-2-7b-chat-hf_wikitext-2-v1-test_bs1xit341618xcont2048_FP8.log
#   ./measure_ppl_slow.py --model=/data/models/llama-2-13b-chat-hf --data=./wikitext/wikitext-2-v1/test-00000-of-00001.parquet --context-size=2048 --batch-size=1 -tp=1 &> ./llama-2-13b-chat-hf_wikitext-2-v1-test_bs1xit341618xcont2048_FP16.log
#   ./measure_ppl_slow.py --model=/data/models/llama-2-7b-chat-hf --data=./wikitext/wikitext-2-v1/test-00000-of-00001.parquet --context-size=2048 --batch-size=1 -tp=1 &> ./llama-2-7b-chat-hf_wikitext-2-v1-test_bs1xit341618xcont2048_FP16.log
#   ./measure_ppl_slow.py --model=/data/models/llama-2-7b-chat-hf --data=./wikitext/wikitext-2-v1/validation-00000-of-00001.parquet --context-size=2048 --batch-size=1 -tp=1 --kv-cache-dtype="fp8" &> ./llama-2-7b-chat-hf_wikitext-2-v1-validation_bs1xit299956xcont2048_FP8.log
#   ./measure_ppl_slow.py --model=/data/models/llama-2-7b-chat-hf --data=./wikitext/wikitext-2-v1/validation-00000-of-00001.parquet --context-size=2048 --batch-size=1 -tp=1 &> ./llama-2-7b-chat-hf_wikitext-2-v1-validation_bs1xit299956xcont2048_FP16.log



#   Configuration                                                       Cross-entropy(nats/tok)     Cross-entropy(bits/tok)     Perplexity(/tok)   Perplexity(/char)
#   llama-2-7b-chat-hf_wikitext-2-v1-test_bs1xit341618xcont2048_FP16    3.0636160192901             4.4198636382178             21.4048            2.3050
#   llama-2-7b-chat-hf_wikitext-2-v1-test_bs1xit341618xcont2048_FP8     3.0636160192901             4.4198636382178             21.4048            2.3050
